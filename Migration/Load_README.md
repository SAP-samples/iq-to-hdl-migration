### Overview
The Load process involves the following operation:
   - The loading of schema and data from HDLFS Object Store to data lake Relational Engine. This operation is performed by the `load_schema_and_data.py` utility.

#### Assumption
- Provision of data lake Relational Engine Instance is complete

#### Prerequisites
The following should be provisioned/installed before the load utilities are run:
    - Python 3.10.x and Above

**_NOTE:_**
- You may need to install additional Python modules required by specific parts of the load scripts.
- If batch-wise extraction is performed, ensure that `Object_Store_Copy_Validation` is set to `No`; otherwise, an error will occur.

#### Input
The files discussed in this section are used as input during the load phase.
Path for json files : `<utility_scripts_dir>/iq-to-hdl-migration/Common`.

  - `load_config.json`: Contains configuration options required to run the load utility via data lake Files.

#### Files used by Load Utility
The following files are used by the `load_schema_and_data.py` utility:
Path for files: `<utility_scripts_dir>/iq-to-hdl-migration/Migration`

- `load_schema.sh`: Loads table schemas and user definitions into the data lake Relational Engine.
- `load_table.sh`:  Loads table data into the data lake Relational Engine.
- `common.py`: Contains common functionalities used by the load utility.

#### Output
The following files and folders are generated as output during the load phase.

##### Folder: `<utility_scripts_dir>/iq-to-hdl-migration/Migration`
###### Generated Files:
- `load_schema_and_data.log`: Contains the log of of LOAD utility execution.

##### Folder:`<Extract_Path>/Migration_Data`:   
###### Generated Files:
- `HDL_Conn_Logs`: Logs directory that contains files <Table_id>_load.log with the DBISQL client connection logs and errors encountered for each table when it is loaded to a data lake Relational Engine instance by the LOAD utility.
- `HDL_LoadSchema.log`: Contains the output of the schema load when the load utility loads the database schema contained in the file AutoUpdated_Reload.sql to the data lake Relational Engine instance. This file also contains connection logs.
- `HDL_LoadFailure.err`: Generated by the LOAD utility. This log file lists/logs all tables in which errors were found while they were being loaded into the data lake Relational Engine instance.
- `HDL_LoadedTables.out`: It contains the list of all tables successfully loaded by the load utility along with the loaded row count.

#### Sample for Linux `load_config.json` file
```
{
"Datalake_Client_Install_Path": "/iqSrver1/install/client",
"Extract_Path": "/iqSrver1/Migration_Extract_Demo",

"HDLFS_Configuration": {
     "Directory_Name" : "Migration_Data",
     "Files_endpoint" : "a1234567-123b-4567-abcd-abcd123456e9.files.hdl.demohdl.com",
     "Cert_path"      : "/iqSrver1/hanadatalake/landscape/client.crt",  
     "Key_path"       : "/iqSrver1/hanadatalake/landscape/client.key"
         },

"HDLADMIN_User": "HDLADMIN",
"HDLADMIN_Pwd": "password12345",

"HDL_Coord_Endpoint": "a1234567-123b-4567-abcd-abcd123456e9-coord.iq.demohdl.com",
"HDL_Worker_Endpoint": "a1234567-123b-4567-abcd-abcd123456e9.iq.demohdl.com",

"HDL_Num_Worker_Conn": 3,
"HDL_Num_Coord_Conn": 1,
"Object_Store_Copy_Validation": "Yes"
}
```

#### Load your extracted schema and data into data lake Relational Engine using the `load_schema_and_data.py` utility.
	Modify the `load_config.json` configuration file. This file is required to run the `load_schema_and_data.py` utility.

	cd `<utility_scripts_dir>/iq-to-hdl-migration/Migration`

        To run this script along with its associated help, run:
            python3 load_schema_and_data.py -h
            OR
            python3 load_schema_and_data.py --help

        Run the following command at the prompt:
        To Load only Schema:
            python3 load_schema_and_data.py --config_file <config file path> --onlyschema y
            OR
	    python3 load_schema_and_data.py -f <config file path> -s y
        To Load only Data:
            python3 load_schema_and_data.py --config_file <config file path> --onlydata y
            OR
	    python3 load_schema_and_data.py -f <config file path> -d y
        To run full load(schema + data in one go):
            python3 load_schema_and_data.py --config_file <config file path> --fullload y
            OR
	    python3 load_schema_and_data.py -f <config file path> -e y

**_NOTE:_**
- Only one of --onlyschema, --onlydata or --fullload can be 'y'. They are mutually exclusive.
- One of the three options must be provided and set to 'y'.
- You can monitor the progress of load by checking (or tail) `<utility_scripts_dir>/iq-to-hdl-migration/Migration/load_schema_and_data.log` file.
- Incase of any load failures in `HDL_LoadFailure.err`, you should rerun `load_schema_and_data.py` in resume mode

#### Cleanup of Data Files from data lake Files Store:

- After the data load into data lake Relational Engine is accomplished successfully, we need to cleanup the Data files(Migration_Data) from HDLFS Object Store using delete command with the HDLFS CLI.

```
(HDLFS Delete command)
hdlfscli -cert <client.crt path>/client.crt -key <client.key path>/client.key -s <files_endpoint>  delete -f /Migration_Data
{"boolean":true}
```
