### Overview
The migration process involves two main operations:
   - The unloading of schema and data to a shared location in the on-premise SAP IQ environment. This operation is performed by the `migration.py` utility.
   - Copy to HDLFS object store.

#### Assumption
- Provision of data lake Relational Engine Instance is complete

#### Prerequisites
The following should be installed/provisioned on the host on which the migration utilities are run:
    - Python 3.10.x and Above
    - Pyodbc(Python module)
    - Minimum Supported IQ Server version - 16.1_SP01
    - Paramiko(Python module)

**_NOTE:_**
You may need to install additional Python modules required by specific parts of the migration scripts.

#### Input
The files discussed in this section are used as input during the migration phase.
Path for json files : `<utility_scripts_dir>/iq-to-hdl-migration/Common`

  - `migration_config.json`: Contains configuration options required to run the migration utility via data lake Files.

#### Files used by Migration Utilities
The following files are used by the `migration.py` utility:
Path for files: `<utility_scripts_dir>/iq-to-hdl-migration/Common`

   - `dbopts_noncustomer.csv`: Used to identify and modify/delete different SAP SQLAnywhere server options which are not supported in data lake Relational Engine.

   - `hosparams_noncustomer.csv`: Used to identify and modify/delete different SAP IQ server options which are not supported in data lake Relational Engine.

   - `DB_Artifacts.list`: Used to identify and modify/delete different artifacts that are not supported in data lake Relational Engine.

   - `login_policy.csv`: Used to identify and modify/delete different login policy-related options that are not supported in data lake Relational Engine.

   - `common.py`: Contains common functionalities used by the migration utility.

#### Output
The following files and folders are generated as output during the migration phase.

##### Folder: `<utility_scripts_dir>/iq-to-hdl-migration/Migration`
###### Generated Files:
- `migration.log`: Contains the log entries generated by the migration utility execution.

##### Folder: `<Extract_Path>/Migration_Data`:
###### Generated Files:
- `reload.sql`: Contains SAP IQ Database schema and its artifacts.
- `iq_tables.list`: Contains the list of all tables along with their row counts and size
- `AutoUpdated_Reload.sql`: The migration utility modifies the reload.sql file by commenting database artifacts which are not supported in the data lake Relational Engine instance and saves it by this name.
- `ExtractedTables.out`: Contains the list of all tables extracted by the migration utility along with the extracted row count.
- `Extracted_Data`: The folder contains the extracted tables. A seperate subfolder labeled with <table_id> is created for each extracted table. Each table-specific folder contains one or more extracted data files preceded by <table_id>.sql. These files contains the appropriate LOAD statement to load the table data into the data lake Relational Engine instance.
- `extractFailure.err`: Error log file generated during the unload phase of migration that lists all tables that could not be extracted due to table unload errors, along with the reason for failure.

**_NOTE:_**
If Batchwise extraction is enabled, then tables are extracted in multiple batches and for each batch `iq_tables_Batch_<batch_num>.list` is created and `ExtractedTables_Batch_<batch_num>.out` is generated.

#### Create your Data Lake Relational Engine Instance and add `HDLFS_Configuration` details in `migration_config.json` file
- Linux Migration: Create your Data Lake Instance
- Windows Migration: Create your Data Lake Instance

#### Install and Configure HDLFSCLI and HDLFS Rest API Endpoint
- Refer to [HDLFS Documentation](https://developers.sap.com/tutorials/data-lake-file-containers-hdlfscli.html)

#### Sample for Linux `migration_config.json` file
```
{
"Datalake_Client_Install_Path": "/iqSrver1/install/client",
"Extract_Path": "/iqSrver1/Migration_Extract_Demo",

"HDLFS_Configuration": {
     "Directory_Name" : "Migration_Data",
     "Files_endpoint" : "a1234567-123b-4567-abcd-abcd123456e9.files.hdl.demohdl.com",
     "Cert_path"      : "/iqSrver1/hanadatalake/landscape/client.crt",
     "Key_path"       : "/iqSrver1/hanadatalake/landscape/client.key"
        },

"Host_Name": "iqSrver",
"Port_Number": 4567,
"DBA_User": "DBA",
"DBA_Pwd": "password",
"Client_Num_Conn": 2,

"IQ_Server_On_Same_Host": "Yes",
"IQ_Host_Login_Id":"",
"IQ_Host_Login_Pwd":"",
"ENC": "tls(fips=yes;tls_type=rsa;skip_certificate_name_check=yes;trusted_certificate=/iqSrver1/iqtesttrust_RSA.pem;identity=/iqSrver1/iqtestcert_RSA.pem;identity_password=test)",
"Batch_Size_GB": 0,
"IQ_Server_Install_Path": "/iqSrver1/install/iq-16.1",
"IQ_Version": "16.1"
}
```

#### Run the migration utility `migration.py` to extract your on-premise SAP IQ schema and data to a folder for upload to data lake Files.

	Modify the `migration_config.json` configuration file. This file is required to run the `migration.py` utility.

	cd `<utility_scripts_dir>/iq-to-hdl-migration/Migration`

        To run this script `migration.py` along with its associated help, run:
                python3 migration.py -h
                OR
                python3 migration.py --help

	If you started the coordinator with the -iqro flag, run the migration.py utility as follows:
		python3 migration.py --config_file <config_file_path>
		OR
		python3 migration.py -f <config_file_path>
	If you started the coordinator without the -iqro flag, run the utility as follows:
		python3 migration.py --config_file <config_file_path> --mode w
		OR
		python3 migration.py -f <config_file_path> -m w

	If you want to unload schema only, run the utility as follows:
		python3 migration.py --config_file <config_file_path> --mode w --onlyschema y
		OR
		python3 migration.py -f <config_file_path> -m w -s y
	Then, to unload data only(resume mode), run:
		python3 migration.py --config_file <config_file_path> --mode w --onlydata y
		OR
		python3 migration.py -f <config_file_path> -m w -d y
	The utility will ask you - Have you already unloaded the schema? (yes/no):
		If yes, the utility will continue. Otherwise it will exit with the error: Please unload schema first before running data-only mode.

	To run full extraction (schema + data in one go):
		python3 migration.py --config_file <config_file_path> --mode w --fullextraction y
		OR
		python3 migration.py -f <config_file_path> -m w -e y

**_NOTE:_**
- Only one of --onlyschema, --onlydata or --fullextraction can be 'y'. They are mutually exclusive.
- One of the three options must be provided and set to 'y'.
- SAP recommends running the coordinator node with `-iqro` flag during migration process.
- You can monitor the progress of migration by checking (or tail) `<utility_scripts_dir>/iq-to-hdl-migration/Migration/migration.log` file.
- Incase of any extraction failures in `extractFailure.err`, you should rerun `migration.py` in resume mode

#### Copy the `<Extract_Path>/Migration_Data` folder to the data lake Files Object Store. This can be accomplished by using copy script i.e. copy_hdlfs.py.

EXAMPLES:
```
(Sample command to copy the data on data lake Files object store)
python3 copy_hdlfs.py --config_file <config_file_path>
OR
python3 copy_hdlfs.py -f <config_file_path>
```
**_NOTE:_**
- config_file_path for copy_hdlfs.py is same config file used for migration.
- You can review `<utility_scripts_dir>/iq-to-hdl-migration/Migration/upload_log_<timestamp>.log for all logs related to upload of data to the HDLFS object store.
- You can check `<utility_scripts_dir>/iq-to-hdl-migration/Migration/successful_uploads.log for all files uploaded successfully to the HDLFS object store.
