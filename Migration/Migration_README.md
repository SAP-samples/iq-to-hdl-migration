1. Overview
The migration process involves two main operations:
   a. Unload of schema and data to a shared location in the on-premise SAP IQ environment. This operation is performed by the migration.py utility.
   b. Load of schema and data into SAP HANA Data Lake. This operation is performed by the load_schema_data.py utility.

2. Prerequisite
The following should be installed on the host on which the migration utilities are run:
    - Python 3
    - Pyodbc

3. Input
The files discussed in this section are taken as input during the pre-migration, migration and load phases.

    - config_aws.json: Configuration options file to provide the options required to run the pre-migration, migration and load utilities. Use this file if your data lake instance is created on Amazon Web Services (AWS).

    - config_azure.json: Configuration options file to provide the options required to run the pre-migration, migration and load utilities. Use this file if your data lake instance is created on Microsoft Azure.

4. Uses
The following files are required by pre-migration and migration.py utilities:

    - dbopts_noncustomer.csv: Used by the pre-migration and migration utilities to identify different SQLAnywhere server options which are not supported in data lake.

    - hosparams_noncustomer.csv: Used by the pre-migration and migration utilities to identify different SAP IQ server options which are not supported in data lake.

    - DB_Artifacts.list: Used by the migration utility to identify different artifacts that are not supported in data lake.

    - login_policy.csv: Used by the migration utility to identify different login policy-related options which are not supported in data lake.

    - common.py: Contains common functionalities used by the pre-migration, migration, and load utilities for example methods to read .json configuration files

5. Output
The files and folders discussed in this section are generated as output during the migration and load phases.

migration.py: Files and folders generated by this utility are as under -

    Following file is generated by migration utility in the folder <Datalake_Client_Install_Path>/IQ-17_1/HDL_Migration/Migration.

         migration.log: Log file that contains the log entries generated by the migration utility execution.

    Following files are generated by Migration utility in the folder  <Extract_Path>/Migration_Data:

    i.   reload.sql: It contains SAP IQ Database schema and its artifacts.

    ii.  iq_tables.list: It contains the list of all tables along with their row counts and size in the SAP IQ database.

    iii. AutoUpdated_Reload.sql: The migration utility modifies the reload.sql file by commenting database artifacts which are not supported in the data lake instance and saves it by this name.

    iv.  ExtractedTables.out: It contains the list of all tables extracted by the migration utility along with the extracted row count.

    v.   Extracted_Data: This folder and its contents are generated by the Migration utility. Within this, a separate folder with name as <table_id> is created for each extracted table.  Each table-specific folder will contain one or more extracted data files preceded by the <table_id>.sql file which has the appropriate LOAD statement that loads the said table data into the data lake instance.

    vi.  Foreign_Key_Constraint.sql: Contains foreign key constraints which would be applied post table data load.

    vii. extractFailure.err: Error log file generated during the unload phase of migration that lists all tables that could not be extracted due to table unload errors, along with the reason for failure.

If Batchwise extraction is enabled, then tables are extracted in multiple batches and for each batch iq_tables_Batch_<batch_num>.list is created and ExtractedTables_Batch_<batch_num>.out is generated.

load_schema_and_data.py: Files and folders generated by this utility are as under -

    Following file is generated by LOAD utility in the folder <Datalake_Client_Install_Path>/IQ-17_1/HDL_Migration/Migration:

         load_schema_and_data.log: Contains the log of of LOAD utilitys execution.

    Following files and folders are generated by the LOAD utility in the folder <Extract_Path>/Migration_Data:

    i.  HDL_Conn_Logs: Logs directory that contains files <Table_id>_load.log with the DBISQL client connection logs and errors encountered for each table when it is loaded to a data lake instance by the LOAD utility.

    ii. HDL_LoadSchema.log: Contains the output of the schema load and Foreign Key Constraint load when the load utility loads the database schema contained in the file AutoUpdated_Reload.sql to the data lake IQ instance. This file also contains connection logs.

    iii. HDL_LoadFailure.err: Generated by the LOAD utility. This log file lists/logs all tables in which errors were found while they were being loaded into the data lake instance.

    iv. HDL_LoadedTables.out: It contains the list of all tables successfully loaded by the load utility along with the loaded row count.

6. Help

Migration_README.md

7. Steps to run the migration utility  migration.py.

i.   Run the following command at the prompt:
python3 migration.py

To run this command along with its associated help, run as follows:

python3 migration.py -h

OR

python3 migration.py --help

ii.  Provide values for the following parameters in the configuration file applicable to you/based on your hyperscaler config_aws.json or config_azure.json. This is required to run the migration.py utility.

   "Host_Name": "<(Coordinator / Simplex server) hostname>",
   "Port_Number": <Port number of (SAP IQ coordinator / SAP IQ simplex server)>,
   "DBA_User": "<DBA username>",
   "DBA_Pwd": "<Optional: DBA user password.Please provide the value or empty string as shown in ReadMe files.>",
   "Extract_Path": "<Shared directory to store the data extracted during migration>",
   "Client_Num_Conn": "<Number of SAP IQ client connections to extract data>",
   "Batch_Size_GB": "<Optional: Batch size in GB if batch extraction is enabled, else set it to 0 or leave this parameter unchanged to go with normal (non-batch) extraction mode. Provide positive integer value without quotes.>"
   "IQ_Server_On_Same_Host": "<SAP IQ Server on same host or not, Valid values:(Yes/No)>",
   "IQ_Host_Login_Id":"<Optional: SAP IQ host login id to establish an  ssh connection if your installations of SAP IQ and the migration utilites (17.1 data lake client installation) are on different hosts.Please provide the value or empty string as shown in ReadMe files.>",
   "IQ_Host_Login_Pwd":"<Optional: SAP IQ host login password to establish an ssh connection if your installations of SAP IQ and the migration utilities (17.1 data lake client installation) are on different hosts.Please provide the value or empty string as shown in ReadMe files.>",
   "IQ_Version": "<Your major SAP IQ version in use: Should be either SAP IQ 16.0 or 16.1>",
   "ENC": "<Optional: ENC string based on Encryption setting on SAP IQ server (None/Simple/TLS(<TLS Options>).Please provide the value or empty string as shown in ReadMe files.>",
   "IQ_Server_Install_Path": "<Path of SAP IQ Server installation directory>",
   "Datalake_Client_Install_Path": "<Path of 17.1 SAP data lake Client Installation directory>",
   "HDLADMIN_User": "<HDLADMIN user name>",
   "HDLADMIN_Pwd": "<Optional: HDLADMIN password.Please provide the value or empty string as shown in ReadMe files.>",

   ## For AWS
   "Hyperscaler_Details": {
      "Name": "Aws",
      "Credentials": {
            "AWS_Access_Key_Id"    : "<Object store Access Key Id>",
            "AWS_Secret_Access_Key": "<Object store Secret Access Key>",
            "AWS_Region"           : "<Object store Region>",
            "Bucket_Name"          : "<Object store Bucket Name>"
      }
   }

   ## For Azure
   "Hyperscaler_Details": {
    "Name": "Azure",
    "Credentials": {
         "Azure_Account_Name": "<Object store Account Name>",
         "Azure_Account_Key" : "<Object store Account Key>",
         "Container_Name"    : "<Object store Container Name>"
     }
   }

iii. source IQ.sh/IQ.csh

Source IQ.sh or IQ.csh from <Datalake_Client_Install_Path> if you are running migration utilities on different host than IQ server host.

Source IQ.sh or IQ.csh from <IQ_Server_Install_Path> if you are running migration utilities on same host as IQ server host.

iv.  If you started the coordinator with the -iqro flag, run the migration.py utility as follows:

python3 migration.py --config_file <config_file_path>
OR
python3 migration.py -f <config_file_path>

If you started the coordinator without the -iqro flag, run the utility as follows:

python3 migration.py --config_file <config_file_path> --mode w
OR
python3 migration.py -f <config_file_path> --mode w

Switch --config_file or -f denote utilizing the config file to access parameters from.
Optional switch \'--mode w\' or \'-m w\' is to run the migration utility when database is in read-write mode.

NOTE:
- SAP recommends to run coordinator node with -iqro flag while migration process.
- You can monitor the progress of migration by checking (or tail) <Datalake_Client_Install_Path>/IQ-17_1/HDL_Migration/Migration/migration.log file.

v.   Copy the Migration_Data folder to your hyperscaler specific object store (AWS or Azure). This can be accomplished by using copy commands for the AWS or Azure CLI.
EXAMPLES:
(AWS copy command)
aws s3 cp <extract_path>/Migration_Data s3://<Object store Bucket Name>/ --recursive

(Azure copy command)
az storage blob upload-batch --connection-string "DefaultEndpointsProtocol=https;AccountName=<Azure Account Name>;AccountKey=<Azure Account Key>;EndpointSuffix=core.windows.net" -d <Object store Container Name> -s <extract_path>

8. Create your data lake IQ instance

Before you load your extracted data into data lake, create a data lake instance.

9. Import/Load your extracted schema into data lake

Steps to run the load_schema_and_data.py utility.

i.  Run the following command at the prompt:
python3 load_schema_and_data.py

To run this command along with its associated help, run as follows:

python3 load_schema_and_data.py -h
OR
python3 load_schema_and_data.py --help

ii. Provide values for the following parameters in the configuration file applicable to you/based on your hyperscaler config_aws.json or config_azure.json. This is required to run the load_schema_and_data.py utility.


   "Extract_Path": "<Shared Directory to store data extracted during migration>",
   "IQ_Version": "<The major SAP IQ version  should be either 16.0 or  16.1>",
   "Datalake_Client_Install_Path": "<Path of 17.1 data lake Client Installation directory>"
   "HDLADMIN_User": "<HDLADMIN user name>",
   "HDLADMIN_Pwd": "<Optional: HDLADMIN password.Please provide the value or empty string as shown in ReadMe files.>",
   "HDL_Coord_Endpoint": "<Coordinator endpoint of the data lake instance>",
   "HDL_Writer_Endpoint": "<Writer endpoint of the data lake instance>",
   "HDL_Num_Writer_Conn": "<Total number of client connections to the writer node in data lake to load data into it>",
   "HDL_Num_Coord_Conn": "<Optional:Total number of client connections to the coordinator node in data lake to load data into it. Provide integer value without quotes or empty string as shown in ReadMe files.>",

   ## For AWS
   "Hyperscaler_Details": {
      "Name": "Aws",
      "Credentials": {
            "AWS_Access_Key_Id"    : "<AWS Access Key Id>",
            "AWS_Secret_Access_Key": "<AWS Secret Access Key>",
            "AWS_Region"           : "<Object store Region>",
            "Bucket_Name"          : "<Object store Bucket Name>"
      }
   }

   ## For Azure
   "Hyperscaler_Details": {
    "Name": "Azure",
    "Credentials": {
         "Azure_Account_Name": "<Azure Account Name>",
         "Azure_Account_Key" : "<Azure Account Key>",
         "Container_Name"    : "<Object store Container Name>"
     }
   }

iii. After adding the required parameters as above, run load_schema_and_data.py along with your hyperscaler-specific configuration file as follows:
python3 load_schema_and_data.py --config_file <config_file_path>
OR
python3 load_schema_and_data.py -f <config_file_path>

Note:
- You can monitor the progress of load on data lake instance by checking (or tail) <Datalake_Client_Install_Path>/IQ-17_1/HDL_Migration/Migration/load_schema_and_data.log file.
- data lake instance writer/coordinator node iqmsg file will have following statement when load query is started on that node.
      [CUST_DATA]:HDL-Migration: LOAD TABLE "<USER>"."<TABLE NAME>"

10. Sample config_azure.json file

    {"Host_Name": "iqSrver",
    "Port_Number": 4567,
    "DBA_User": "DBA",
    "DBA_Pwd": "password",
    "Extract_Path": "/iqSrver1/Migration_Extract_Demo",
    "Client_Num_Conn": 2,
    "Batch_Size_GB": 100,
    "IQ_Server_On_Same_Host": "Yes",
    "IQ_Host_Login_Id":"",
    "IQ_Host_Login_Pwd":"",
    "IQ_Version": "16.1",
    "ENC": "tls(fips=yes;tls_type=rsa;skip_certificate_name_check=yes;trusted_certificate=/iqSrver1/iqtesttrust_RSA.pem;identity=/iqSrver1/iqtestcert_RSA.pem;identity_password=test)",
    "IQ_Server_Install_Path": "/iqSrver1/install/iq-16.1",
    "Datalake_Client_Install_Path": "/iqSrver1/install/client",
    "HDLADMIN_User": "HDLADMIN",
    "HDLADMIN_Pwd": "password12345",
    "HDL_Coord_Endpoint": "a1234567-123b-4567-abcd-abcd123456e9-coord.iq.demohdl.com",
    "HDL_Writer_Endpoint": "a1234567-123b-4567-abcd-abcd123456e9.iq.demohdl.com",
    "HDL_Num_Writer_Conn": 2,
    "HDL_Num_Coord_Conn": 0,
    "Object_Store_Copy_Validation": "No",
    "Hyperscaler_Details": {
        "Name": "Azure",
        "Credentials": {
            "Azure_Account_Name": "AzureBlobStore",
            "Azure_Account_Key" : "aBCDefghijKL01234mnopQRST879jklmxyz+YYcd",
            "Container_Name"    : "migration-demo"
            }
        }
    }

11. Sample config_aws.json file

    {"Host_Name": "iqSrver",
    "Port_Number": 4567,
    "DBA_User": "DBA",
    "DBA_Pwd": "password",
    "Extract_Path": "/iqSrver1/Migration_Extract_Demo",
    "Client_Num_Conn": 2,
    "Batch_Size_GB": 100,
    "IQ_Server_On_Same_Host": "Yes",
    "IQ_Host_Login_Id":"",
    "IQ_Host_Login_Pwd":"",
    "IQ_Version": "16.1",
    "ENC": "tls(fips=yes;tls_type=rsa;skip_certificate_name_check=yes;trusted_certificate=/iqSrver1/iqtesttrust_RSA.pem;identity=/iqSrver1/iqtestcert_RSA.pem;identity_password=test)",
    "IQ_Server_Install_Path": "/iqSrver1/install/iq-16.1",
    "Datalake_Client_Install_Path": "/iqSrver1/install/client",
    "HDLADMIN_User": "HDLADMIN",
    "HDLADMIN_Pwd": "password12345",
    "HDL_Coord_Endpoint": "a1234567-123b-4567-abcd-abcd123456e9-coord.iq.demohdl.com",
    "HDL_Writer_Endpoint": "a1234567-123b-4567-abcd-abcd123456e9.iq.demohdl.com",
    "HDL_Num_Writer_Conn": 2,
    "HDL_Num_Coord_Conn": 0,
    "Object_Store_Copy_Validation": "No",
    "Hyperscaler_Details": {
        "Name": "Aws",
        "Credentials": {
            "AWS_Access_Key_Id": "ABCDEFGHIJKLMNOPQRS1",
            "AWS_Secret_Access_Key" : "aBCDefghijKL01234mnopQRST879jklmxyz+YYcd",
            "AWS_Region" : "us-east-1",
            "Bucket_Name" : "migration-demo"
            }
        }
    }
